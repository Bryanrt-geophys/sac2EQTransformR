21/03/30 21:15:20 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
21/03/30 21:15:21 INFO SparkContext: Running Spark version 2.4.3
21/03/30 21:15:21 INFO SparkContext: Submitted application: sparklyr
21/03/30 21:15:21 INFO SecurityManager: Changing view acls to: jolante
21/03/30 21:15:21 INFO SecurityManager: Changing modify acls to: jolante
21/03/30 21:15:21 INFO SecurityManager: Changing view acls groups to: 
21/03/30 21:15:21 INFO SecurityManager: Changing modify acls groups to: 
21/03/30 21:15:21 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(jolante); groups with view permissions: Set(); users  with modify permissions: Set(jolante); groups with modify permissions: Set()
21/03/30 21:15:21 INFO Utils: Successfully started service 'sparkDriver' on port 52285.
21/03/30 21:15:21 INFO SparkEnv: Registering MapOutputTracker
21/03/30 21:15:21 INFO SparkEnv: Registering BlockManagerMaster
21/03/30 21:15:21 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
21/03/30 21:15:21 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
21/03/30 21:15:21 INFO DiskBlockManager: Created local directory at /private/var/folders/ht/rsh9r06x72j66qz7cn7mpx0m0000gp/T/blockmgr-4bf491c1-5a38-4315-bb84-1d59b783af86
21/03/30 21:15:21 INFO MemoryStore: MemoryStore started with capacity 912.3 MB
21/03/30 21:15:21 INFO SparkEnv: Registering OutputCommitCoordinator
21/03/30 21:15:21 INFO Utils: Successfully started service 'SparkUI' on port 4040.
21/03/30 21:15:21 INFO SparkUI: Bound SparkUI to 127.0.0.1, and started at http://localhost:4040
21/03/30 21:15:21 INFO SparkContext: Added JAR file:/Library/Frameworks/R.framework/Versions/4.0/Resources/library/sparklyr/java/sparklyr-2.4-2.11.jar at spark://localhost:52285/jars/sparklyr-2.4-2.11.jar with timestamp 1617160521857
21/03/30 21:15:21 INFO Executor: Starting executor ID driver on host localhost
21/03/30 21:15:21 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 52286.
21/03/30 21:15:22 INFO NettyBlockTransferService: Server created on localhost:52286
21/03/30 21:15:22 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
21/03/30 21:15:22 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, localhost, 52286, None)
21/03/30 21:15:22 INFO BlockManagerMasterEndpoint: Registering block manager localhost:52286 with 912.3 MB RAM, BlockManagerId(driver, localhost, 52286, None)
21/03/30 21:15:22 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, localhost, 52286, None)
21/03/30 21:15:22 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, localhost, 52286, None)
21/03/30 21:15:22 INFO SharedState: loading hive config file: file:/Users/jolante/spark/spark-2.4.3-bin-hadoop2.7/conf/hive-site.xml
21/03/30 21:15:22 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/Users/jolante/Bryan/ML_work/sac2eqtransformr/spark-warehouse').
21/03/30 21:15:22 INFO SharedState: Warehouse path is 'file:/Users/jolante/Bryan/ML_work/sac2eqtransformr/spark-warehouse'.
21/03/30 21:15:22 INFO StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint
21/03/30 21:15:25 INFO HiveUtils: Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
21/03/30 21:15:25 INFO HiveMetaStore: 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
21/03/30 21:15:25 INFO ObjectStore: ObjectStore, initialize called
21/03/30 21:15:25 INFO Persistence: Property hive.metastore.integral.jdo.pushdown unknown - will be ignored
21/03/30 21:15:25 INFO Persistence: Property datanucleus.cache.level2 unknown - will be ignored
21/03/30 21:15:27 INFO ObjectStore: Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
21/03/30 21:15:27 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
21/03/30 21:15:27 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
21/03/30 21:15:28 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
21/03/30 21:15:28 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
21/03/30 21:15:28 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
21/03/30 21:15:28 INFO ObjectStore: Initialized ObjectStore
21/03/30 21:15:28 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 1.2.0
21/03/30 21:15:28 WARN ObjectStore: Failed to get database default, returning NoSuchObjectException
21/03/30 21:15:28 INFO HiveMetaStore: Added admin role in metastore
21/03/30 21:15:28 INFO HiveMetaStore: Added public role in metastore
21/03/30 21:15:28 INFO HiveMetaStore: No user is added in admin role, since config is empty
21/03/30 21:15:28 INFO HiveMetaStore: 0: get_all_databases
21/03/30 21:15:28 INFO audit: ugi=jolante	ip=unknown-ip-addr	cmd=get_all_databases	
21/03/30 21:15:28 INFO HiveMetaStore: 0: get_functions: db=default pat=*
21/03/30 21:15:28 INFO audit: ugi=jolante	ip=unknown-ip-addr	cmd=get_functions: db=default pat=*	
21/03/30 21:15:28 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MResourceUri" is tagged as "embedded-only" so does not have its own datastore table.
21/03/30 21:15:28 INFO SessionState: Created HDFS directory: /tmp/hive/jolante
21/03/30 21:15:29 INFO SessionState: Created local directory: /var/folders/ht/rsh9r06x72j66qz7cn7mpx0m0000gp/T/jolante
21/03/30 21:15:29 INFO SessionState: Created local directory: /var/folders/ht/rsh9r06x72j66qz7cn7mpx0m0000gp/T/f01dab9f-798b-4593-b753-3e6398eaa472_resources
21/03/30 21:15:29 INFO SessionState: Created HDFS directory: /tmp/hive/jolante/f01dab9f-798b-4593-b753-3e6398eaa472
21/03/30 21:15:29 INFO SessionState: Created local directory: /var/folders/ht/rsh9r06x72j66qz7cn7mpx0m0000gp/T/jolante/f01dab9f-798b-4593-b753-3e6398eaa472
21/03/30 21:15:29 INFO SessionState: Created HDFS directory: /tmp/hive/jolante/f01dab9f-798b-4593-b753-3e6398eaa472/_tmp_space.db
21/03/30 21:15:29 INFO HiveClientImpl: Warehouse location for Hive client (version 1.2.2) is file:/Users/jolante/Bryan/ML_work/sac2eqtransformr/spark-warehouse
21/03/30 21:15:29 INFO HiveMetaStore: 0: get_database: default
21/03/30 21:15:29 INFO audit: ugi=jolante	ip=unknown-ip-addr	cmd=get_database: default	
21/03/30 21:15:29 INFO HiveMetaStore: 0: get_database: global_temp
21/03/30 21:15:29 INFO audit: ugi=jolante	ip=unknown-ip-addr	cmd=get_database: global_temp	
21/03/30 21:15:29 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException
21/03/30 21:15:29 INFO HiveMetaStore: 0: get_database: default
21/03/30 21:15:29 INFO audit: ugi=jolante	ip=unknown-ip-addr	cmd=get_database: default	
21/03/30 21:15:29 INFO HiveMetaStore: 0: get_database: default
21/03/30 21:15:29 INFO audit: ugi=jolante	ip=unknown-ip-addr	cmd=get_database: default	
21/03/30 21:15:29 INFO HiveMetaStore: 0: get_tables: db=default pat=*
21/03/30 21:15:29 INFO audit: ugi=jolante	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
21/03/30 21:15:29 INFO CodeGenerator: Code generated in 184.452817 ms
21/03/30 21:15:29 INFO CodeGenerator: Code generated in 15.993391 ms
21/03/30 21:15:29 INFO CodeGenerator: Code generated in 9.341318 ms
21/03/30 21:15:30 INFO ContextCleaner: Cleaned accumulator 1
21/03/30 21:15:30 INFO SparkContext: Starting job: count at utils.scala:135
21/03/30 21:15:30 INFO DAGScheduler: Registering RDD 3 (count at utils.scala:135)
21/03/30 21:15:30 INFO DAGScheduler: Got job 0 (count at utils.scala:135) with 1 output partitions
21/03/30 21:15:30 INFO DAGScheduler: Final stage: ResultStage 1 (count at utils.scala:135)
21/03/30 21:15:30 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 0)
21/03/30 21:15:30 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 0)
21/03/30 21:15:30 INFO DAGScheduler: Submitting ShuffleMapStage 0 (MapPartitionsRDD[3] at count at utils.scala:135), which has no missing parents
21/03/30 21:15:30 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 7.9 KB, free 912.3 MB)
21/03/30 21:15:30 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 4.2 KB, free 912.3 MB)
21/03/30 21:15:30 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on localhost:52286 (size: 4.2 KB, free: 912.3 MB)
21/03/30 21:15:30 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1161
21/03/30 21:15:30 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[3] at count at utils.scala:135) (first 15 tasks are for partitions Vector(0))
21/03/30 21:15:30 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks
21/03/30 21:15:30 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7882 bytes)
21/03/30 21:15:30 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
21/03/30 21:15:30 INFO Executor: Fetching spark://localhost:52285/jars/sparklyr-2.4-2.11.jar with timestamp 1617160521857
21/03/30 21:15:30 INFO TransportClientFactory: Successfully created connection to localhost/127.0.0.1:52285 after 18 ms (0 ms spent in bootstraps)
21/03/30 21:15:30 INFO Utils: Fetching spark://localhost:52285/jars/sparklyr-2.4-2.11.jar to /private/var/folders/ht/rsh9r06x72j66qz7cn7mpx0m0000gp/T/spark-e2feec5d-5c80-4a6f-bdee-521ef487c9d2/userFiles-d1ca4a11-6151-4ad7-9288-57453a9b1a07/fetchFileTemp2390424989983523923.tmp
21/03/30 21:15:30 INFO Executor: Adding file:/private/var/folders/ht/rsh9r06x72j66qz7cn7mpx0m0000gp/T/spark-e2feec5d-5c80-4a6f-bdee-521ef487c9d2/userFiles-d1ca4a11-6151-4ad7-9288-57453a9b1a07/sparklyr-2.4-2.11.jar to class loader
21/03/30 21:15:30 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1542 bytes result sent to driver
21/03/30 21:15:30 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 231 ms on localhost (executor driver) (1/1)
21/03/30 21:15:30 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
21/03/30 21:15:30 INFO DAGScheduler: ShuffleMapStage 0 (count at utils.scala:135) finished in 0.532 s
21/03/30 21:15:30 INFO DAGScheduler: looking for newly runnable stages
21/03/30 21:15:30 INFO DAGScheduler: running: Set()
21/03/30 21:15:30 INFO DAGScheduler: waiting: Set(ResultStage 1)
21/03/30 21:15:30 INFO DAGScheduler: failed: Set()
21/03/30 21:15:30 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[6] at count at utils.scala:135), which has no missing parents
21/03/30 21:15:30 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 7.1 KB, free 912.3 MB)
21/03/30 21:15:30 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 3.8 KB, free 912.3 MB)
21/03/30 21:15:30 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on localhost:52286 (size: 3.8 KB, free: 912.3 MB)
21/03/30 21:15:30 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1161
21/03/30 21:15:30 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at count at utils.scala:135) (first 15 tasks are for partitions Vector(0))
21/03/30 21:15:30 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks
21/03/30 21:15:30 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, NODE_LOCAL, 7767 bytes)
21/03/30 21:15:30 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
21/03/30 21:15:30 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks
21/03/30 21:15:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 7 ms
21/03/30 21:15:30 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1775 bytes result sent to driver
21/03/30 21:15:30 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 46 ms on localhost (executor driver) (1/1)
21/03/30 21:15:30 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
21/03/30 21:15:30 INFO DAGScheduler: ResultStage 1 (count at utils.scala:135) finished in 0.058 s
21/03/30 21:15:30 INFO DAGScheduler: Job 0 finished: count at utils.scala:135, took 0.662029 s
21/03/30 21:15:30 INFO HiveMetaStore: 0: get_database: default
21/03/30 21:15:30 INFO audit: ugi=jolante	ip=unknown-ip-addr	cmd=get_database: default	
21/03/30 21:15:30 INFO HiveMetaStore: 0: get_database: default
21/03/30 21:15:30 INFO audit: ugi=jolante	ip=unknown-ip-addr	cmd=get_database: default	
21/03/30 21:15:30 INFO HiveMetaStore: 0: get_tables: db=default pat=*
21/03/30 21:15:30 INFO audit: ugi=jolante	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
21/03/30 21:15:30 INFO SparkContext: Starting job: count at utils.scala:135
21/03/30 21:15:30 INFO DAGScheduler: Registering RDD 10 (count at utils.scala:135)
21/03/30 21:15:30 INFO DAGScheduler: Got job 1 (count at utils.scala:135) with 1 output partitions
21/03/30 21:15:30 INFO DAGScheduler: Final stage: ResultStage 3 (count at utils.scala:135)
21/03/30 21:15:30 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 2)
21/03/30 21:15:30 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 2)
21/03/30 21:15:30 INFO DAGScheduler: Submitting ShuffleMapStage 2 (MapPartitionsRDD[10] at count at utils.scala:135), which has no missing parents
21/03/30 21:15:30 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 7.9 KB, free 912.3 MB)
21/03/30 21:15:30 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 4.2 KB, free 912.3 MB)
21/03/30 21:15:30 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on localhost:52286 (size: 4.2 KB, free: 912.3 MB)
21/03/30 21:15:30 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1161
21/03/30 21:15:30 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[10] at count at utils.scala:135) (first 15 tasks are for partitions Vector(0))
21/03/30 21:15:30 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks
21/03/30 21:15:30 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2, localhost, executor driver, partition 0, PROCESS_LOCAL, 7882 bytes)
21/03/30 21:15:30 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
21/03/30 21:15:30 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 1499 bytes result sent to driver
21/03/30 21:15:30 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 7 ms on localhost (executor driver) (1/1)
21/03/30 21:15:30 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool 
21/03/30 21:15:30 INFO DAGScheduler: ShuffleMapStage 2 (count at utils.scala:135) finished in 0.013 s
21/03/30 21:15:30 INFO DAGScheduler: looking for newly runnable stages
21/03/30 21:15:30 INFO DAGScheduler: running: Set()
21/03/30 21:15:30 INFO DAGScheduler: waiting: Set(ResultStage 3)
21/03/30 21:15:30 INFO DAGScheduler: failed: Set()
21/03/30 21:15:30 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[13] at count at utils.scala:135), which has no missing parents
21/03/30 21:15:30 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 7.1 KB, free 912.3 MB)
21/03/30 21:15:30 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 3.8 KB, free 912.3 MB)
21/03/30 21:15:30 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on localhost:52286 (size: 3.8 KB, free: 912.3 MB)
21/03/30 21:15:30 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1161
21/03/30 21:15:30 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[13] at count at utils.scala:135) (first 15 tasks are for partitions Vector(0))
21/03/30 21:15:30 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks
21/03/30 21:15:30 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3, localhost, executor driver, partition 0, NODE_LOCAL, 7767 bytes)
21/03/30 21:15:30 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)
21/03/30 21:15:30 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks
21/03/30 21:15:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
21/03/30 21:15:30 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 1775 bytes result sent to driver
21/03/30 21:15:30 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 7 ms on localhost (executor driver) (1/1)
21/03/30 21:15:30 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool 
21/03/30 21:15:30 INFO DAGScheduler: ResultStage 3 (count at utils.scala:135) finished in 0.012 s
21/03/30 21:15:30 INFO DAGScheduler: Job 1 finished: count at utils.scala:135, took 0.028784 s
21/03/30 21:16:24 INFO ContextCleaner: Cleaned accumulator 98
21/03/30 21:16:24 INFO ContextCleaner: Cleaned accumulator 73
21/03/30 21:16:24 INFO ContextCleaner: Cleaned accumulator 95
21/03/30 21:16:24 INFO ContextCleaner: Cleaned accumulator 118
21/03/30 21:16:24 INFO ContextCleaner: Cleaned accumulator 89
21/03/30 21:16:24 INFO ContextCleaner: Cleaned accumulator 125
21/03/30 21:16:24 INFO ContextCleaner: Cleaned accumulator 100
21/03/30 21:16:24 INFO ContextCleaner: Cleaned accumulator 106
21/03/30 21:16:24 INFO ContextCleaner: Cleaned accumulator 128
21/03/30 21:16:24 INFO ContextCleaner: Cleaned accumulator 92
21/03/30 21:16:24 INFO ContextCleaner: Cleaned shuffle 1
21/03/30 21:16:24 INFO ContextCleaner: Cleaned accumulator 120
21/03/30 21:16:24 INFO ContextCleaner: Cleaned accumulator 93
21/03/30 21:16:24 INFO ContextCleaner: Cleaned accumulator 109
21/03/30 21:16:24 INFO ContextCleaner: Cleaned accumulator 104
21/03/30 21:16:24 INFO ContextCleaner: Cleaned accumulator 126
21/03/30 21:16:24 INFO ContextCleaner: Cleaned accumulator 82
21/03/30 21:16:24 INFO ContextCleaner: Cleaned accumulator 107
21/03/30 21:16:24 INFO ContextCleaner: Cleaned accumulator 122
21/03/30 21:16:24 INFO ContextCleaner: Cleaned accumulator 79
21/03/30 21:16:24 INFO ContextCleaner: Cleaned accumulator 111
21/03/30 21:16:24 INFO ContextCleaner: Cleaned accumulator 112
21/03/30 21:16:24 INFO ContextCleaner: Cleaned accumulator 71
21/03/30 21:16:24 INFO ContextCleaner: Cleaned accumulator 78
21/03/30 21:16:24 INFO ContextCleaner: Cleaned accumulator 66
21/03/30 21:16:24 INFO ContextCleaner: Cleaned accumulator 69
21/03/30 21:16:24 INFO ContextCleaner: Cleaned accumulator 94
21/03/30 21:16:24 INFO BlockManagerInfo: Removed broadcast_1_piece0 on localhost:52286 in memory (size: 3.8 KB, free: 912.3 MB)
21/03/30 21:16:24 INFO ContextCleaner: Cleaned accumulator 123
21/03/30 21:16:24 INFO ContextCleaner: Cleaned accumulator 116
21/03/30 21:16:24 INFO ContextCleaner: Cleaned accumulator 72
21/03/30 21:16:24 INFO ContextCleaner: Cleaned accumulator 74
21/03/30 21:16:24 INFO ContextCleaner: Cleaned accumulator 117
21/03/30 21:16:24 INFO ContextCleaner: Cleaned accumulator 88
21/03/30 21:16:24 INFO ContextCleaner: Cleaned accumulator 129
21/03/30 21:16:24 INFO ContextCleaner: Cleaned accumulator 119
21/03/30 21:16:24 INFO ContextCleaner: Cleaned accumulator 81
21/03/30 21:16:24 INFO ContextCleaner: Cleaned accumulator 97
21/03/30 21:16:24 INFO ContextCleaner: Cleaned accumulator 99
21/03/30 21:16:24 INFO ContextCleaner: Cleaned accumulator 105
21/03/30 21:16:24 INFO ContextCleaner: Cleaned accumulator 127
21/03/30 21:16:24 INFO BlockManagerInfo: Removed broadcast_2_piece0 on localhost:52286 in memory (size: 4.2 KB, free: 912.3 MB)
21/03/30 21:16:24 INFO ContextCleaner: Cleaned accumulator 80
21/03/30 21:16:24 INFO ContextCleaner: Cleaned accumulator 67
21/03/30 21:16:24 INFO ContextCleaner: Cleaned accumulator 77
21/03/30 21:16:24 INFO ContextCleaner: Cleaned accumulator 75
21/03/30 21:16:24 INFO ContextCleaner: Cleaned accumulator 87
21/03/30 21:16:24 INFO ContextCleaner: Cleaned accumulator 108
21/03/30 21:16:24 INFO ContextCleaner: Cleaned accumulator 113
21/03/30 21:16:24 INFO ContextCleaner: Cleaned accumulator 96
21/03/30 21:16:24 INFO BlockManagerInfo: Removed broadcast_3_piece0 on localhost:52286 in memory (size: 3.8 KB, free: 912.3 MB)
21/03/30 21:16:24 INFO ContextCleaner: Cleaned accumulator 103
21/03/30 21:16:24 INFO ContextCleaner: Cleaned accumulator 68
21/03/30 21:16:24 INFO ContextCleaner: Cleaned accumulator 131
21/03/30 21:16:24 INFO ContextCleaner: Cleaned accumulator 91
21/03/30 21:16:24 INFO ContextCleaner: Cleaned accumulator 84
21/03/30 21:16:24 INFO ContextCleaner: Cleaned accumulator 124
21/03/30 21:16:24 INFO ContextCleaner: Cleaned accumulator 114
21/03/30 21:16:24 INFO ContextCleaner: Cleaned accumulator 102
21/03/30 21:16:24 INFO ContextCleaner: Cleaned accumulator 110
21/03/30 21:16:24 INFO ContextCleaner: Cleaned accumulator 76
21/03/30 21:16:24 INFO ContextCleaner: Cleaned accumulator 85
21/03/30 21:16:24 INFO ContextCleaner: Cleaned accumulator 83
21/03/30 21:16:24 INFO ContextCleaner: Cleaned accumulator 86
21/03/30 21:16:24 INFO ContextCleaner: Cleaned accumulator 115
21/03/30 21:16:24 INFO ContextCleaner: Cleaned accumulator 90
21/03/30 21:16:24 INFO ContextCleaner: Cleaned accumulator 70
21/03/30 21:16:24 INFO ContextCleaner: Cleaned accumulator 101
21/03/30 21:16:24 INFO ContextCleaner: Cleaned accumulator 121
21/03/30 21:16:24 INFO ContextCleaner: Cleaned accumulator 130
21/03/30 21:19:43 INFO SparkContext: Invoking stop() from shutdown hook
21/03/30 21:19:43 INFO SparkUI: Stopped Spark web UI at http://localhost:4040
21/03/30 21:19:43 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
21/03/30 21:19:43 INFO MemoryStore: MemoryStore cleared
21/03/30 21:19:43 INFO BlockManager: BlockManager stopped
21/03/30 21:19:43 INFO BlockManagerMaster: BlockManagerMaster stopped
21/03/30 21:19:43 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
21/03/30 21:19:43 INFO SparkContext: Successfully stopped SparkContext
21/03/30 21:19:43 INFO ShutdownHookManager: Shutdown hook called
21/03/30 21:19:43 INFO ShutdownHookManager: Deleting directory /private/var/folders/ht/rsh9r06x72j66qz7cn7mpx0m0000gp/T/spark-e88cab89-3169-4807-9a92-a79348b67968
21/03/30 21:19:43 INFO ShutdownHookManager: Deleting directory /private/var/folders/ht/rsh9r06x72j66qz7cn7mpx0m0000gp/T/spark-e2feec5d-5c80-4a6f-bdee-521ef487c9d2
21/03/30 21:20:23 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
21/03/30 21:20:24 INFO SparkContext: Running Spark version 2.4.3
21/03/30 21:20:24 INFO SparkContext: Submitted application: sparklyr
21/03/30 21:20:24 INFO SecurityManager: Changing view acls to: jolante
21/03/30 21:20:24 INFO SecurityManager: Changing modify acls to: jolante
21/03/30 21:20:24 INFO SecurityManager: Changing view acls groups to: 
21/03/30 21:20:24 INFO SecurityManager: Changing modify acls groups to: 
21/03/30 21:20:24 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(jolante); groups with view permissions: Set(); users  with modify permissions: Set(jolante); groups with modify permissions: Set()
21/03/30 21:20:24 INFO Utils: Successfully started service 'sparkDriver' on port 52605.
21/03/30 21:20:24 INFO SparkEnv: Registering MapOutputTracker
21/03/30 21:20:24 INFO SparkEnv: Registering BlockManagerMaster
21/03/30 21:20:24 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
21/03/30 21:20:24 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
21/03/30 21:20:24 INFO DiskBlockManager: Created local directory at /private/var/folders/ht/rsh9r06x72j66qz7cn7mpx0m0000gp/T/blockmgr-06d02579-8f33-4a32-9571-c0b70e20b4b9
21/03/30 21:20:24 INFO MemoryStore: MemoryStore started with capacity 912.3 MB
21/03/30 21:20:24 INFO SparkEnv: Registering OutputCommitCoordinator
21/03/30 21:20:24 INFO Utils: Successfully started service 'SparkUI' on port 4040.
21/03/30 21:20:24 INFO SparkUI: Bound SparkUI to 127.0.0.1, and started at http://localhost:4040
21/03/30 21:20:24 INFO SparkContext: Added JAR file:/Library/Frameworks/R.framework/Versions/4.0/Resources/library/sparklyr/java/sparklyr-2.4-2.11.jar at spark://localhost:52605/jars/sparklyr-2.4-2.11.jar with timestamp 1617160824745
21/03/30 21:20:24 INFO Executor: Starting executor ID driver on host localhost
21/03/30 21:20:24 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 52606.
21/03/30 21:20:24 INFO NettyBlockTransferService: Server created on localhost:52606
21/03/30 21:20:24 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
21/03/30 21:20:24 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, localhost, 52606, None)
21/03/30 21:20:24 INFO BlockManagerMasterEndpoint: Registering block manager localhost:52606 with 912.3 MB RAM, BlockManagerId(driver, localhost, 52606, None)
21/03/30 21:20:24 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, localhost, 52606, None)
21/03/30 21:20:24 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, localhost, 52606, None)
21/03/30 21:20:25 INFO SharedState: loading hive config file: file:/Users/jolante/spark/spark-2.4.3-bin-hadoop2.7/conf/hive-site.xml
21/03/30 21:20:25 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/Users/jolante/Bryan/ML_work/sac2eqtransformr/spark-warehouse').
21/03/30 21:20:25 INFO SharedState: Warehouse path is 'file:/Users/jolante/Bryan/ML_work/sac2eqtransformr/spark-warehouse'.
21/03/30 21:20:25 INFO StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint
21/03/30 21:20:27 INFO HiveUtils: Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
21/03/30 21:20:27 INFO HiveMetaStore: 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
21/03/30 21:20:27 INFO ObjectStore: ObjectStore, initialize called
21/03/30 21:20:27 INFO Persistence: Property hive.metastore.integral.jdo.pushdown unknown - will be ignored
21/03/30 21:20:27 INFO Persistence: Property datanucleus.cache.level2 unknown - will be ignored
21/03/30 21:20:28 INFO ObjectStore: Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
21/03/30 21:20:29 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
21/03/30 21:20:29 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
21/03/30 21:20:30 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
21/03/30 21:20:30 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
21/03/30 21:20:30 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
21/03/30 21:20:30 INFO ObjectStore: Initialized ObjectStore
21/03/30 21:20:30 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 1.2.0
21/03/30 21:20:30 WARN ObjectStore: Failed to get database default, returning NoSuchObjectException
21/03/30 21:20:30 INFO HiveMetaStore: Added admin role in metastore
21/03/30 21:20:30 INFO HiveMetaStore: Added public role in metastore
21/03/30 21:20:30 INFO HiveMetaStore: No user is added in admin role, since config is empty
21/03/30 21:20:30 INFO HiveMetaStore: 0: get_all_databases
21/03/30 21:20:30 INFO audit: ugi=jolante	ip=unknown-ip-addr	cmd=get_all_databases	
21/03/30 21:20:30 INFO HiveMetaStore: 0: get_functions: db=default pat=*
21/03/30 21:20:30 INFO audit: ugi=jolante	ip=unknown-ip-addr	cmd=get_functions: db=default pat=*	
21/03/30 21:20:30 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MResourceUri" is tagged as "embedded-only" so does not have its own datastore table.
21/03/30 21:20:30 INFO SessionState: Created local directory: /var/folders/ht/rsh9r06x72j66qz7cn7mpx0m0000gp/T/79ddbed0-403c-4cb9-9f00-36cdb3c99bc4_resources
21/03/30 21:20:30 INFO SessionState: Created HDFS directory: /tmp/hive/jolante/79ddbed0-403c-4cb9-9f00-36cdb3c99bc4
21/03/30 21:20:30 INFO SessionState: Created local directory: /var/folders/ht/rsh9r06x72j66qz7cn7mpx0m0000gp/T/jolante/79ddbed0-403c-4cb9-9f00-36cdb3c99bc4
21/03/30 21:20:30 INFO SessionState: Created HDFS directory: /tmp/hive/jolante/79ddbed0-403c-4cb9-9f00-36cdb3c99bc4/_tmp_space.db
21/03/30 21:20:30 INFO HiveClientImpl: Warehouse location for Hive client (version 1.2.2) is file:/Users/jolante/Bryan/ML_work/sac2eqtransformr/spark-warehouse
21/03/30 21:20:30 INFO HiveMetaStore: 0: get_database: default
21/03/30 21:20:30 INFO audit: ugi=jolante	ip=unknown-ip-addr	cmd=get_database: default	
21/03/30 21:20:30 INFO HiveMetaStore: 0: get_database: global_temp
21/03/30 21:20:30 INFO audit: ugi=jolante	ip=unknown-ip-addr	cmd=get_database: global_temp	
21/03/30 21:20:30 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException
21/03/30 21:20:30 INFO HiveMetaStore: 0: get_database: default
21/03/30 21:20:30 INFO audit: ugi=jolante	ip=unknown-ip-addr	cmd=get_database: default	
21/03/30 21:20:30 INFO HiveMetaStore: 0: get_database: default
21/03/30 21:20:30 INFO audit: ugi=jolante	ip=unknown-ip-addr	cmd=get_database: default	
21/03/30 21:20:30 INFO HiveMetaStore: 0: get_tables: db=default pat=*
21/03/30 21:20:30 INFO audit: ugi=jolante	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
21/03/30 21:20:31 INFO CodeGenerator: Code generated in 150.686662 ms
21/03/30 21:20:31 INFO CodeGenerator: Code generated in 17.226384 ms
21/03/30 21:20:31 INFO CodeGenerator: Code generated in 11.09663 ms
21/03/30 21:20:31 INFO SparkContext: Starting job: count at utils.scala:135
21/03/30 21:20:31 INFO DAGScheduler: Registering RDD 3 (count at utils.scala:135)
21/03/30 21:20:31 INFO DAGScheduler: Got job 0 (count at utils.scala:135) with 1 output partitions
21/03/30 21:20:31 INFO DAGScheduler: Final stage: ResultStage 1 (count at utils.scala:135)
21/03/30 21:20:31 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 0)
21/03/30 21:20:31 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 0)
21/03/30 21:20:31 INFO DAGScheduler: Submitting ShuffleMapStage 0 (MapPartitionsRDD[3] at count at utils.scala:135), which has no missing parents
21/03/30 21:20:31 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 7.9 KB, free 912.3 MB)
21/03/30 21:20:31 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 4.2 KB, free 912.3 MB)
21/03/30 21:20:31 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on localhost:52606 (size: 4.2 KB, free: 912.3 MB)
21/03/30 21:20:31 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1161
21/03/30 21:20:31 INFO ContextCleaner: Cleaned accumulator 1
21/03/30 21:20:31 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[3] at count at utils.scala:135) (first 15 tasks are for partitions Vector(0))
21/03/30 21:20:31 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks
21/03/30 21:20:31 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7882 bytes)
21/03/30 21:20:31 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
21/03/30 21:20:31 INFO Executor: Fetching spark://localhost:52605/jars/sparklyr-2.4-2.11.jar with timestamp 1617160824745
21/03/30 21:20:32 INFO TransportClientFactory: Successfully created connection to localhost/127.0.0.1:52605 after 111 ms (0 ms spent in bootstraps)
21/03/30 21:20:32 INFO Utils: Fetching spark://localhost:52605/jars/sparklyr-2.4-2.11.jar to /private/var/folders/ht/rsh9r06x72j66qz7cn7mpx0m0000gp/T/spark-ccecc55f-88c8-478c-ae44-39c14b628b6d/userFiles-2e27fe9e-4199-4b96-a66d-78deaaa28d3c/fetchFileTemp6492298851237293386.tmp
21/03/30 21:20:32 INFO Executor: Adding file:/private/var/folders/ht/rsh9r06x72j66qz7cn7mpx0m0000gp/T/spark-ccecc55f-88c8-478c-ae44-39c14b628b6d/userFiles-2e27fe9e-4199-4b96-a66d-78deaaa28d3c/sparklyr-2.4-2.11.jar to class loader
21/03/30 21:20:32 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1585 bytes result sent to driver
21/03/30 21:20:32 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 305 ms on localhost (executor driver) (1/1)
21/03/30 21:20:32 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
21/03/30 21:20:32 INFO DAGScheduler: ShuffleMapStage 0 (count at utils.scala:135) finished in 0.558 s
21/03/30 21:20:32 INFO DAGScheduler: looking for newly runnable stages
21/03/30 21:20:32 INFO DAGScheduler: running: Set()
21/03/30 21:20:32 INFO DAGScheduler: waiting: Set(ResultStage 1)
21/03/30 21:20:32 INFO DAGScheduler: failed: Set()
21/03/30 21:20:32 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[6] at count at utils.scala:135), which has no missing parents
21/03/30 21:20:32 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 7.1 KB, free 912.3 MB)
21/03/30 21:20:32 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 3.8 KB, free 912.3 MB)
21/03/30 21:20:32 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on localhost:52606 (size: 3.8 KB, free: 912.3 MB)
21/03/30 21:20:32 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1161
21/03/30 21:20:32 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at count at utils.scala:135) (first 15 tasks are for partitions Vector(0))
21/03/30 21:20:32 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks
21/03/30 21:20:32 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, NODE_LOCAL, 7767 bytes)
21/03/30 21:20:32 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
21/03/30 21:20:32 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks
21/03/30 21:20:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 7 ms
21/03/30 21:20:32 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1818 bytes result sent to driver
21/03/30 21:20:32 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 43 ms on localhost (executor driver) (1/1)
21/03/30 21:20:32 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
21/03/30 21:20:32 INFO DAGScheduler: ResultStage 1 (count at utils.scala:135) finished in 0.054 s
21/03/30 21:20:32 INFO DAGScheduler: Job 0 finished: count at utils.scala:135, took 0.678329 s
21/03/30 21:20:32 INFO HiveMetaStore: 0: get_database: default
21/03/30 21:20:32 INFO audit: ugi=jolante	ip=unknown-ip-addr	cmd=get_database: default	
21/03/30 21:20:32 INFO HiveMetaStore: 0: get_database: default
21/03/30 21:20:32 INFO audit: ugi=jolante	ip=unknown-ip-addr	cmd=get_database: default	
21/03/30 21:20:32 INFO HiveMetaStore: 0: get_tables: db=default pat=*
21/03/30 21:20:32 INFO audit: ugi=jolante	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
21/03/30 21:20:32 INFO SparkContext: Starting job: count at utils.scala:135
21/03/30 21:20:32 INFO DAGScheduler: Registering RDD 10 (count at utils.scala:135)
21/03/30 21:20:32 INFO DAGScheduler: Got job 1 (count at utils.scala:135) with 1 output partitions
21/03/30 21:20:32 INFO DAGScheduler: Final stage: ResultStage 3 (count at utils.scala:135)
21/03/30 21:20:32 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 2)
21/03/30 21:20:32 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 2)
21/03/30 21:20:32 INFO DAGScheduler: Submitting ShuffleMapStage 2 (MapPartitionsRDD[10] at count at utils.scala:135), which has no missing parents
21/03/30 21:20:32 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 7.9 KB, free 912.3 MB)
21/03/30 21:20:32 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 4.2 KB, free 912.3 MB)
21/03/30 21:20:32 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on localhost:52606 (size: 4.2 KB, free: 912.3 MB)
21/03/30 21:20:32 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1161
21/03/30 21:20:32 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[10] at count at utils.scala:135) (first 15 tasks are for partitions Vector(0))
21/03/30 21:20:32 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks
21/03/30 21:20:32 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2, localhost, executor driver, partition 0, PROCESS_LOCAL, 7882 bytes)
21/03/30 21:20:32 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
21/03/30 21:20:32 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 1499 bytes result sent to driver
21/03/30 21:20:32 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 11 ms on localhost (executor driver) (1/1)
21/03/30 21:20:32 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool 
21/03/30 21:20:32 INFO DAGScheduler: ShuffleMapStage 2 (count at utils.scala:135) finished in 0.016 s
21/03/30 21:20:32 INFO DAGScheduler: looking for newly runnable stages
21/03/30 21:20:32 INFO DAGScheduler: running: Set()
21/03/30 21:20:32 INFO DAGScheduler: waiting: Set(ResultStage 3)
21/03/30 21:20:32 INFO DAGScheduler: failed: Set()
21/03/30 21:20:32 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[13] at count at utils.scala:135), which has no missing parents
21/03/30 21:20:32 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 7.1 KB, free 912.3 MB)
21/03/30 21:20:32 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 3.8 KB, free 912.3 MB)
21/03/30 21:20:32 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on localhost:52606 (size: 3.8 KB, free: 912.3 MB)
21/03/30 21:20:32 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1161
21/03/30 21:20:32 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[13] at count at utils.scala:135) (first 15 tasks are for partitions Vector(0))
21/03/30 21:20:32 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks
21/03/30 21:20:32 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3, localhost, executor driver, partition 0, NODE_LOCAL, 7767 bytes)
21/03/30 21:20:32 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)
21/03/30 21:20:32 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks
21/03/30 21:20:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
21/03/30 21:20:32 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 1775 bytes result sent to driver
21/03/30 21:20:32 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 6 ms on localhost (executor driver) (1/1)
21/03/30 21:20:32 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool 
21/03/30 21:20:32 INFO DAGScheduler: ResultStage 3 (count at utils.scala:135) finished in 0.012 s
21/03/30 21:20:32 INFO DAGScheduler: Job 1 finished: count at utils.scala:135, took 0.033982 s
21/03/30 21:37:32 INFO SparkContext: Invoking stop() from shutdown hook
21/03/30 21:37:32 INFO SparkUI: Stopped Spark web UI at http://localhost:4040
21/03/30 21:37:32 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
21/03/30 21:37:32 INFO MemoryStore: MemoryStore cleared
21/03/30 21:37:32 INFO BlockManager: BlockManager stopped
21/03/30 21:37:32 INFO BlockManagerMaster: BlockManagerMaster stopped
21/03/30 21:37:32 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
21/03/30 21:37:32 INFO SparkContext: Successfully stopped SparkContext
21/03/30 21:37:32 INFO ShutdownHookManager: Shutdown hook called
21/03/30 21:37:32 INFO ShutdownHookManager: Deleting directory /private/var/folders/ht/rsh9r06x72j66qz7cn7mpx0m0000gp/T/spark-88ae643c-838e-4735-897a-aecb02969d1f
21/03/30 21:37:32 INFO ShutdownHookManager: Deleting directory /private/var/folders/ht/rsh9r06x72j66qz7cn7mpx0m0000gp/T/spark-ccecc55f-88c8-478c-ae44-39c14b628b6d
